<hr>
<h2 id="title-10-11月进度总结-检索方面date-2023-11-18-11-36-56tags-Transformer-Clip-Video-Retrieval-Image-Retrieval-多模态categories-每月总结excerpt-总结一下网路框架"><a href="#title-10-11月进度总结-检索方面date-2023-11-18-11-36-56tags-Transformer-Clip-Video-Retrieval-Image-Retrieval-多模态categories-每月总结excerpt-总结一下网路框架" class="headerlink" title="title: 10-11月进度总结 检索方面date: 2023-11-18 11:36:56tags:- Transformer- Clip- Video Retrieval- Image Retrieval- 多模态categories: - 每月总结excerpt: 总结一下网路框架"></a>title: 10-11月进度总结 检索方面<br>date: 2023-11-18 11:36:56<br>tags:<br>- Transformer<br>- Clip<br>- Video Retrieval<br>- Image Retrieval<br>- 多模态<br>categories:<br> - 每月总结<br>excerpt: 总结一下网路框架</h2><h1 id="Video-Retrieval"><a href="#Video-Retrieval" class="headerlink" title="Video Retrieval"></a>Video Retrieval</h1><h2 id="Space-Time-Attention-基本框架-ICML-2021"><a href="#Space-Time-Attention-基本框架-ICML-2021" class="headerlink" title="Space-Time Attention(基本框架  ICML 2021)"></a>Space-Time Attention(基本框架  ICML 2021)</h2><p><strong><a href="(https://arxiv.org/abs/2102.05095)">Is Space-Time Attention All You Need for Video Understanding?</a></strong></p>
<p><img src="/4.1.1.png" alt="image-20230811163530398"></p>
<ul>
<li>直接将Attention应用到图片的方法迁移到视频之中（空间注意力）</li>
<li>在时间上和空间上分别做三个自注意力机制，进行融合</li>
<li><strong>拆分为空间和时间上分别进行注意力机制计算（时间-&gt; 空间）</strong>文章提出</li>
<li>local global拆分（在局部进行注意力计算）</li>
<li>沿着特定的轴进行注意力计算（将三维拆分为三个一维进行注意力机制计算）</li>
</ul>
<p><img src="/4.2.1.png" alt="image-20230811163643822"></p>
<p>&emsp;想法简单、效果好、容易迁移、可以用于处理超过1min的视频</p>
<h2 id="CLIP4Clip-2021"><a href="#CLIP4Clip-2021" class="headerlink" title="CLIP4Clip(2021)"></a>CLIP4Clip(2021)</h2><p><a href="https://arxiv.org/abs/2104.08860"><strong>An Empirical Study of CLIP for End to End Video Clip Retrieval</strong></a></p>
<p><img src="/image-20231118101514862.png" alt="image-20231118101514862"></p>
<h2 id="FT-ICCV-2021"><a href="#FT-ICCV-2021" class="headerlink" title="FT(ICCV 2021)"></a>FT(ICCV 2021)</h2><p><a href="https://arxiv.org/abs/2104.00650">Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval</a></p>
<p><img src="/image-20240623003403700.png" alt="image-20240623003403700"></p>
<h2 id="X-Pool-CVPR-2022"><a href="#X-Pool-CVPR-2022" class="headerlink" title="X-Pool(CVPR 2022)"></a>X-Pool(CVPR 2022)</h2><p><a href="https://arxiv.org/abs/2203.15086"><strong>Cross-Modal Language-Video Attention for Text-Video Retrieval</strong></a></p>
<p>&emsp;选择与文本相关的文本Token相近的视觉Token</p>
<p><img src="/image-20231118100200730.png" alt="image-20231118100200730"></p>
<h2 id="X-Clip-ECCV-2022"><a href="#X-Clip-ECCV-2022" class="headerlink" title="X-Clip(ECCV 2022)"></a>X-Clip(ECCV 2022)</h2><p><a href="https://arxiv.org/abs/2208.02816"><strong>Expanding Language-Image Pretrained Models for General Video Recognition</strong></a></p>
<p>&emsp;时间维度融合的新方法，先融合到时间维度再利用融合后的特征返回和图像维度融合</p>
<p><img src="/image-20231118100600504.png" alt="image-20231118100600504"></p>
<p><img src="/image-20231118100630051.png" alt="image-20231118100630051"></p>
<h2 id="TS2-Net-ECCV-2022"><a href="#TS2-Net-ECCV-2022" class="headerlink" title="TS2-Net(ECCV 2022)"></a>TS2-Net(ECCV 2022)</h2><p><strong>TS2-Net: Token Shift and Selection Transformer for Text-Video Retrieval</strong></p>
<p>&emsp;时间维度融合，利用Token之间的平移捕捉变动差别</p>
<p><img src="/image-20231118100927351.png" alt="image-20231118100927351"></p>
<p><img src="/../../../../AppData/Roaming/Typora/typora-user-images/image-20240623003309770.png" alt="image-20240623003309770"></p>
<p><img src="/image-20231118100942559.png" alt="image-20231118100942559"></p>
<h2 id="EMCL-NeurIPS-2022"><a href="#EMCL-NeurIPS-2022" class="headerlink" title="EMCL(NeurIPS 2022)"></a>EMCL(NeurIPS 2022)</h2><p><a href="https://arxiv.org/abs/2211.11427"><strong>Expectation-Maximization Contrastive Learning for Compact Video-and-Language Representations</strong></a></p>
<p><img src="/image-20231118101102419.png" alt="image-20231118101102419"></p>
<h2 id="DRL-2022"><a href="#DRL-2022" class="headerlink" title="DRL(2022)"></a>DRL(2022)</h2><p><a href="https://arxiv.org/abs/2203.07111"><strong>Disentangled Representation Learning for Text-Video Retrieval</strong></a></p>
<p><img src="/image-20240623004337676.png" alt="image-20240623004337676"></p>
<p><img src="/image-20240623003827210.png" alt="image-20231118101328656"></p>
<p><img src="/image-20231118101340854.png" alt="image-20231118101340854"></p>
<h2 id="CenterCLIP-SIGIR-2022"><a href="#CenterCLIP-SIGIR-2022" class="headerlink" title="CenterCLIP(SIGIR 2022)"></a>CenterCLIP(SIGIR 2022)</h2><p><a href="https://arxiv.org/abs/2205.00823"><strong>Token Clustering for Efficient Text-Video Retrieval</strong></a></p>
<p><img src="/image-20240623004241975.png" alt="image-20240623004241975"></p>
<p>&emsp;去除冗余信息 </p>
<p><img src="/image-20231118101632756.png" alt="image-20231118101632756"></p>
<h2 id="TMVPM-NIPS-2022"><a href="#TMVPM-NIPS-2022" class="headerlink" title="TMVPM(NIPS 2022)"></a>TMVPM(NIPS 2022)</h2><p><a href="https://arxiv.org/abs/2209.13307">Text-Adaptive Multiple Visual Prototype Matching for Video-Text Retrieval</a></p>
<p><img src="/image-20240623004410389.png" alt="image-20240623004410389"></p>
<p><img src="/image-20240623004905507.png" alt="image-20240623004905507"></p>
<h2 id="CLIP-bnl-ACMMM-2022"><a href="#CLIP-bnl-ACMMM-2022" class="headerlink" title="CLIP-bnl(ACMMM 2022)"></a>CLIP-bnl(ACMMM 2022)</h2><p><a href="https://arxiv.org/abs/2205.00132">Learn to Understand Negation in Video Retrieval</a></p>
<p><img src="/image-20240623005007065.png" alt="image-20240623005007065"></p>
<h2 id="Prompt-Switch-ICCV-2023"><a href="#Prompt-Switch-ICCV-2023" class="headerlink" title="Prompt Switch(ICCV 2023)"></a>Prompt Switch(ICCV 2023)</h2><p><a href="https://arxiv.org/abs/2308.07648">Prompt Switch: Efficient CLIP Adaptation for Text-Video Retrieval</a></p>
<p><img src="/image-20231118101724910.png" alt="image-20231118101724910"></p>
<p><img src="/image-20240623005105355.png" alt="image-20240623005105355"></p>
<p><img src="/image-20231118101753781.png" alt="image-20231118101753781"></p>
<h2 id="Cap4Video-CVPR-2023"><a href="#Cap4Video-CVPR-2023" class="headerlink" title="Cap4Video(CVPR 2023)"></a>Cap4Video(CVPR 2023)</h2><p><a href="https://arxiv.org/abs/2301.00184"><strong>Cap4Video What Can Auxiliary Captions Do for Text-Video Retrieval</strong></a></p>
<p>&emsp;利用</p>
<p><img src="/image-20231118102034385.png" alt="image-20231118102034385"></p>
<p><img src="/image-20231118102051534.png" alt="image-20231118102051534"></p>
<h2 id="UATVR-ICCV-2023"><a href="#UATVR-ICCV-2023" class="headerlink" title="UATVR(ICCV 2023)"></a>UATVR(ICCV 2023)</h2><p><a href="https://arxiv.org/abs/2301.06309"> <strong>UATVR: Uncertainty-Adaptive Text-Video Retrieval</strong></a></p>
<p>&emsp;粗细粒度的偏重就有不确定性，本文针对这一问题，提出了不确定性的自适应检索方法</p>
<p><img src="/image-20231118112636621.png" alt="image-20231118112636621"></p>
<h2 id="UCOFIA-ICCV-2023"><a href="#UCOFIA-ICCV-2023" class="headerlink" title="UCOFIA(ICCV 2023)"></a>UCOFIA(ICCV 2023)</h2><p><a href="https://arxiv.org/abs/2309.10091"><strong>Unified Coarse-to-Fine Alignment for Video-Text Retrieval</strong></a></p>
<p>&emsp;如何规范的利用视频和文本进行粗细粒度对齐，减轻不想关信息的影响</p>
<p><img src="/image-20231118113036134.png" alt="image-20231118113036134"></p>
<p><img src="/image-20231118113204634.png" alt="image-20231118113204634"></p>
<h2 id="ProST-ICCV-2023"><a href="#ProST-ICCV-2023" class="headerlink" title="ProST(ICCV 2023)"></a>ProST(ICCV 2023)</h2><p><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Progressive_Spatio-Temporal_Prototype_Matching_for_Text-Video_Retrieval_ICCV_2023_paper.pdf">Progressive Spatio-Temporal Prototype Matching for Text-Video Retrieval</a></p>
<p><img src="/image-20240623005421731.png" alt="image-20240623005421731"></p>
<p><img src="/image-20240623005517179.png" alt="image-20240623005517179"></p>
<h2 id="DiCoSA-IJCAI-2023"><a href="#DiCoSA-IJCAI-2023" class="headerlink" title="DiCoSA(IJCAI 2023)"></a>DiCoSA(IJCAI 2023)</h2><p><a href="https://arxiv.org/abs/2305.12218">Text-Video Retrieval with Disentangled Conceptualization and Set-to-Set</a></p>
<p><img src="/image-20240623005931675.png" alt="image-20240623005931675"></p>
<p><img src="/image-20240623010014920.png" alt="image-20240623010014920"></p>
<h2 id="SViTT-CVPR-2023"><a href="#SViTT-CVPR-2023" class="headerlink" title="SViTT(CVPR 2023)"></a>SViTT(CVPR 2023)</h2><p><a href="https://arxiv.org/abs/2304.08809">SViTT: Temporal Learning of Sparse Video-Text Transformers</a></p>
<p><img src="/image-20240623010054378.png" alt="image-20240623010054378"></p>
<p><img src="/image-20240623010142798.png" alt="image-20240623010142798"></p>
<h2 id="Text-Is-MASS-CVPR-2024"><a href="#Text-Is-MASS-CVPR-2024" class="headerlink" title="Text Is MASS(CVPR 2024)"></a>Text Is MASS(CVPR 2024)</h2><p><a href="https://arxiv.org/abs/2403.17998">Text Is MASS: Modeling as Stochastic Embedding for Text-Video Retrieval</a></p>
<p><img src="/image-20240623010258465.png" alt="image-20240623010258465"></p>
<p><img src="/image-20240623010321544.png" alt="image-20240623010321544"></p>
<p><img src="/image-20240623010421388.png" alt="image-20240623010421388"></p>
<h1 id="CLIP"><a href="#CLIP" class="headerlink" title="CLIP"></a>CLIP</h1><h2 id="A-Picture-is-Worth-More-Than-77-Text-Tokens-Evaluating-CLIP-Style-Models-on-Dense-Captions-CVPR-2024"><a href="#A-Picture-is-Worth-More-Than-77-Text-Tokens-Evaluating-CLIP-Style-Models-on-Dense-Captions-CVPR-2024" class="headerlink" title="A Picture is Worth More Than 77 Text Tokens: Evaluating CLIP-Style Models on Dense Captions(CVPR 2024)"></a>A Picture is Worth More Than 77 Text Tokens: Evaluating CLIP-Style Models on Dense Captions(CVPR 2024)</h2><p><a href="https://arxiv.org/abs/2312.08578">A Picture is Worth More Than 77 Text Tokens: Evaluating CLIP-Style Models on Dense Captions</a></p>
<h1 id="General-Methods"><a href="#General-Methods" class="headerlink" title="General Methods"></a>General Methods</h1><h2 id="Cross-Modal-Retrieval-with-Querybank-Normalisation-CVPR-2022"><a href="#Cross-Modal-Retrieval-with-Querybank-Normalisation-CVPR-2022" class="headerlink" title="Cross Modal Retrieval with Querybank Normalisation(CVPR 2022)"></a>Cross Modal Retrieval with Querybank Normalisation(CVPR 2022)</h2><p><a href="https://arxiv.org/abs/2112.12777">Cross Modal Retrieval with Querybank Normalisation</a></p>
<p>&emsp;主要解决的是检索中某一个样本对大部分相似度过高的情况</p>
<p><img src="/image-20231118102559410.png" alt="image-20231118102559410"></p>
<p><img src="/image-20231118102620534.png" alt="image-20231118102620534"></p>
<p><img src="/image-20231118102640705.png" alt="image-20231118102640705"></p>
